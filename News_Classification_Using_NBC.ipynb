{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News-Classification-Using-NBC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8jT_0o_-bqV",
        "colab_type": "text"
      },
      "source": [
        "**P.S.:**\n",
        "\n",
        "To perform text Classification of News Headlines and classify news into different topics for a News Website"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQPGq9Rp-KZZ",
        "colab_type": "code",
        "outputId": "8c1f5e19-2e87-457e-9dd3-d6ec3e029058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "# Loading the necessary libraries\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "data=fetch_20newsgroups()\n",
        "data.target_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1QvPGB-_ep4",
        "colab_type": "code",
        "outputId": "28220d33-94fd-4efd-e002-c2b42a7890db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Defining all the categories\n",
        "\n",
        "categories=['alt.atheism',\n",
        " 'comp.graphics',\n",
        " 'comp.os.ms-windows.misc',\n",
        " 'comp.sys.ibm.pc.hardware',\n",
        " 'comp.sys.mac.hardware',\n",
        " 'comp.windows.x',\n",
        " 'misc.forsale',\n",
        " 'rec.autos',\n",
        " 'rec.motorcycles',\n",
        " 'rec.sport.baseball',\n",
        " 'rec.sport.hockey',\n",
        " 'sci.crypt',\n",
        " 'sci.electronics',\n",
        " 'sci.med',\n",
        " 'sci.space',\n",
        " 'soc.religion.christian',\n",
        " 'talk.politics.guns',\n",
        " 'talk.politics.mideast',\n",
        " 'talk.politics.misc',\n",
        " 'talk.religion.misc']\n",
        "# training the data on these categories\n",
        "train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "\n",
        "# testing the data for these categories\n",
        "test=fetch_20newsgroups(subset='test', categories=categories)\n",
        "\n",
        "# print a data\n",
        "print(train.data[5])\n",
        "print(train.target[5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\n",
            "Subject: Re: Rewording the Second Amendment (ideas)\n",
            "Organization: VTT\n",
            "Lines: 58\n",
            "\n",
            "In article <1r1eu1$4t@transfer.stratus.com> cdt@sw.stratus.com (C. D. Tavares) writes:\n",
            ">In article <1993Apr20.083057.16899@ousrvr.oulu.fi>, dfo@vttoulu.tko.vtt.fi (Foxvog Douglas) writes:\n",
            ">> In article <1qv87v$4j3@transfer.stratus.com> cdt@sw.stratus.com (C. D. Tavares) writes:\n",
            ">> >In article <C5n3GI.F8F@ulowell.ulowell.edu>, jrutledg@cs.ulowell.edu (John Lawrence Rutledge) writes:\n",
            ">\n",
            ">> >> The massive destructive power of many modern weapons, makes the\n",
            ">> >> cost of an accidental or crimial usage of these weapons to great.\n",
            ">> >> The weapons of mass destruction need to be in the control of\n",
            ">> >> the government only.  Individual access would result in the\n",
            ">> >> needless deaths of millions.  This makes the right of the people\n",
            ">> >> to keep and bear many modern weapons non-existant.\n",
            "\n",
            ">> >Thanks for stating where you're coming from.  Needless to say, I\n",
            ">> >disagree on every count.\n",
            "\n",
            ">> You believe that individuals should have the right to own weapons of\n",
            ">> mass destruction?  I find it hard to believe that you would support a \n",
            ">> neighbor's right to keep nuclear weapons, biological weapons, and nerve\n",
            ">> gas on his/her property.  \n",
            "\n",
            ">> If we cannot even agree on keeping weapons of mass destruction out of\n",
            ">> the hands of individuals, can there be any hope for us?\n",
            "\n",
            ">I don't sign any blank checks.\n",
            "\n",
            "Of course.  The term must be rigidly defined in any bill.\n",
            "\n",
            ">When Doug Foxvog says \"weapons of mass destruction,\" he means CBW and\n",
            ">nukes.  When Sarah Brady says \"weapons of mass destruction\" she means\n",
            ">Street Sweeper shotguns and semi-automatic SKS rifles.  \n",
            "\n",
            "I doubt she uses this term for that.  You are using a quote allegedly\n",
            "from her, can you back it up?\n",
            "\n",
            ">When John\n",
            ">Lawrence Rutledge says \"weapons of mass destruction,\" and then immediately\n",
            ">follows it with:\n",
            "\n",
            ">>> The US has thousands of people killed each year by handguns,\n",
            ">>> this number can easily be reduced by putting reasonable restrictions\n",
            ">>> on them.\n",
            "\n",
            ">...what does Rutledge mean by the term?\n",
            "\n",
            "I read the article as presenting first an argument about weapons of mass\n",
            "destruction (as commonly understood) and then switching to other topics.\n",
            "The first point evidently was to show that not all weapons should be\n",
            "allowed, and then the later analysis was, given this understanding, to\n",
            "consider another class.\n",
            "\n",
            ">cdt@rocket.sw.stratus.com   --If you believe that I speak for my company,\n",
            ">OR cdt@vos.stratus.com        write today for my special Investors' Packet...\n",
            "\n",
            "\n",
            "\n",
            "-- \n",
            "doug foxvog\n",
            "douglas.foxvog@vtt.fi\n",
            "\n",
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvFBgg2LI-4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import necessary packages\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline # we are using pipeline because its a huge data\n",
        "\n",
        "#created a model based on Multinomial NB\n",
        "model=make_pipeline(TfidfVectorizer(),MultinomialNB())\n",
        "\n",
        "# training the model with the train data\n",
        "model.fit(train.data,train.target)\n",
        "\n",
        "# predictions on test data\n",
        "labels=model.predict(test.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQam2iP7LyVn",
        "colab_type": "code",
        "outputId": "4e4cd4e9-499a-4cb1-cfb3-445033c05c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "# creating confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "accuracy=accuracy_score(test.target,labels)\n",
        "mat=confusion_matrix(test.target,labels)\n",
        "print(mat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[166   0   0   1   0   1   0   0   1   1   1   3   0   6   3 123   4   8\n",
            "    0   1]\n",
            " [  1 252  15  12   9  18   1   2   1   5   2  41   4   0   6  15   4   1\n",
            "    0   0]\n",
            " [  0  14 258  45   3   9   0   2   1   3   2  25   1   0   6  23   2   0\n",
            "    0   0]\n",
            " [  0   5  11 305  17   1   3   6   1   0   2  19  13   0   5   3   1   0\n",
            "    0   0]\n",
            " [  0   3   8  23 298   0   3   8   1   3   1  16   8   0   2   8   3   0\n",
            "    0   0]\n",
            " [  1  21  17  13   2 298   1   0   1   1   0  23   0   1   4  10   2   0\n",
            "    0   0]\n",
            " [  0   1   3  31  12   1 271  19   4   4   6   5  12   6   3   9   3   0\n",
            "    0   0]\n",
            " [  0   1   0   3   0   0   4 364   3   2   2   4   1   1   3   3   4   0\n",
            "    1   0]\n",
            " [  0   0   0   1   0   0   2  10 371   0   0   4   0   0   0   8   2   0\n",
            "    0   0]\n",
            " [  0   0   0   0   1   0   0   4   0 357  22   0   0   0   2   9   1   1\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   1   0   4 387   1   0   0   1   5   0   0\n",
            "    0   0]\n",
            " [  0   2   1   0   0   1   1   3   0   0   0 383   1   0   0   3   1   0\n",
            "    0   0]\n",
            " [  0   4   2  17   5   0   2   8   7   1   2  78 235   3  11  15   2   1\n",
            "    0   0]\n",
            " [  2   3   0   1   1   3   1   0   2   3   4  11   5 292   6  52   6   4\n",
            "    0   0]\n",
            " [  0   2   0   1   0   3   0   2   1   0   1   6   1   2 351  19   4   0\n",
            "    1   0]\n",
            " [  2   0   0   0   0   0   0   0   1   0   0   0   0   1   2 392   0   0\n",
            "    0   0]\n",
            " [  0   0   0   1   0   0   2   0   1   1   0  10   0   0   1   6 341   1\n",
            "    0   0]\n",
            " [  0   1   0   0   0   0   0   0   0   1   0   2   0   0   0  24   3 344\n",
            "    1   0]\n",
            " [  2   0   0   0   0   0   0   1   0   0   1  11   0   1   7  35 118   5\n",
            "  129   0]\n",
            " [ 33   2   0   0   0   0   0   0   0   1   1   3   0   4   4 131  29   5\n",
            "    3  35]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ceMHRO9MzDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing category on new data based on trained model\n",
        "def predict_category(s, train=train, model=model):\n",
        "  pred= model.predict([s])\n",
        "  return train.target_names[pred[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyRyjxiOkVEm",
        "colab_type": "code",
        "outputId": "2bb2647a-25eb-4401-c161-3366125ff681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# print accuracy of model\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7738980350504514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx8EF-PRlBGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "clf = make_pipeline(TfidfVectorizer(),SVC(gamma='auto'))\n",
        "clf.fit(train.data,train.target) \n",
        "pred_svm=clf.predict(test.data)\n",
        "accuracy_svm=accuracy_score(test.target,pred_svm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msz5T1VzmC9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(accuracy_svm) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCnLVVxtoUJk",
        "colab_type": "code",
        "outputId": "b89ae6d1-ed33-430f-f5a5-ce41f2af713b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = make_pipeline(TfidfVectorizer(),RandomForestClassifier(n_estimators=100,random_state=0)) \n",
        "clf.fit(train.data,train.target)  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token...\n",
              "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
              "                                        criterion='gini', max_depth=None,\n",
              "                                        max_features='auto',\n",
              "                                        max_leaf_nodes=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=None,\n",
              "                                        oob_score=False, random_state=0,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZDif-fpsHLn",
        "colab_type": "code",
        "outputId": "11c4a4d3-db07-44f9-d619-95577855bf04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_rf=clf.predict(test.data)\n",
        "accuracy_rf=accuracy_score(test.target,pred_rf)\n",
        "print(accuracy_rf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.765400955921402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMI0x_dYuK-Q",
        "colab_type": "code",
        "outputId": "4356e74f-547a-4bea-eddf-7eda4b1cd271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# So here NB classifier perform better\n",
        "\n",
        "predict_category('Jesus Christ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'soc.religion.christian'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSdcuUZFuuK-",
        "colab_type": "code",
        "outputId": "4bb72b90-de92-464a-a60a-fb473b755c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict_category('Mr. Narendra Modi is prime minister of India')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.politics.mideast'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG_wNEXbu6wB",
        "colab_type": "code",
        "outputId": "a9f43a73-846f-4db4-dd46-1981de207a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict_category('As the reach of data science expands across the globe, developed, developing or underdeveloped, all countries are looking forward to enhance their data-friendly infrastructure, capacity and workforce. Especially in United Kingdoms, the technology is at rise with increasing demand for data science professionals. A recent report revealed that around 80 percent of the UK companies planning to hire data scientists or seek data consultancy in coming years. Several companies are seeking specialist skills and expertise to help navigate the potentials of uncertain market conditions.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sci.crypt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULvkZU0pvXY-",
        "colab_type": "code",
        "outputId": "6527e7a7-c68d-46e4-9f70-02f3453402ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict_category('The Indian Premier League is the most celebrated T20 event around the globe. With each passing year, its popularity is increasing exponentially. Here everything is top-notch from world-class players to renowned commentators. Even the IPL cheerleaders who entertain the audience with their special acts should be given their share of the credit. Their combined efforts have played a big role in making this tournament a huge success.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rec.sport.hockey'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gao1rPQGy48z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}